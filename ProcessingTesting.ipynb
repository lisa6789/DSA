{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tika import parser\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import RegexpTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('maxent_treebank_pos_tagger')\n",
    "#get the standard-tagger\n",
    "t0 = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle');\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "regexp_tagger = RegexpTagger(\n",
    "            [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "             (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "             (r'.*able$', 'JJ'),                # adjectives\n",
    "             (r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "             (r'.*ly$', 'RB'),                  # adverbs\n",
    "             (r'.*s$', 'NNS'),                  # plural nouns\n",
    "             (r'.*ing$', 'VBG'),                # gerunds\n",
    "             (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "             (r'.*', 'NN')                      # nouns (default)\n",
    "        ])\n",
    "brown_train = brown.tagged_sents(categories='news')\n",
    "unigram_tagger = UnigramTagger(brown_train, backoff=regexp_tagger)\n",
    "#unigram-tagger with t0 as backoff\n",
    "t1 = nltk.UnigramTagger(brown_train,backoff=t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define location of files and keywords - TODO parameterise these\n",
    "stemmer = SnowballStemmer('english')\n",
    "pstemmer = PorterStemmer()\n",
    "\n",
    "input_path = 'C:\\\\test'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "keywords = ['terrorism', 'IS', 'bomb', 'is', 'the', 'consortium']\n",
    "poskeywords = unigram_tagger.tag(keywords)\n",
    "posk1 = t1.tag(keywords)\n",
    "posk2 = nltk.pos_tag(keywords)\n",
    "stemkeywords = unigram_tagger.tag([pstemmer.stem(t) for t in keywords])\n",
    "snowstemkeywords = unigram_tagger.tag([stemmer.stem(t) for t in keywords])\n",
    "\n",
    "# Set up Dataframe\n",
    "d = pd.DataFrame()\n",
    "\n",
    "# Create a list to use for clustering\n",
    "doclist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrorism', 'NN'),\n",
       " ('IS', 'NN'),\n",
       " ('bomb', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('the', 'AT'),\n",
       " ('consortium', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poskeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrorism', 'NN'),\n",
       " ('IS', 'VBZ'),\n",
       " ('bomb', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('the', 'AT'),\n",
       " ('consortium', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posk1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrorism', 'NN'),\n",
       " ('IS', 'NNP'),\n",
       " ('bomb', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('consortium', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemkeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tika to parse the file\n",
    "def parsewithtika(inputfile):\n",
    "    parsed = parser.from_file(inputfile)\n",
    "    # Extract the text content from the parsed file\n",
    "    psd = parsed[\"content\"]\n",
    "    return re.sub(r'\\s+', ' ', psd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return NLTK text from the document - used to filter out short documents but may\n",
    "# also be used for further processing in future dev\n",
    "def tokenmakerwords(inputfile):\n",
    "    # Create tokens\n",
    "    tokens = word_tokenize(inputfile)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    stripped = [w.strip(string.punctuation) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    text = nltk.Text(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language filter\n",
    "def filterlanguage(inputfile):\n",
    "    if detect(inputfile) != 'en':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokens, parts of speech tagging\n",
    "def wordtokens(dataframe):\n",
    "    dataframe['words'] = (dataframe['sentences'].apply(lambda x: [word_tokenize(item) for item in x]))\n",
    "    dataframe['pos'] = dataframe['words'].apply(lambda x: [st.tag(item) for item in x])\n",
    "    dataframe['allwords'] = dataframe['words'].apply(lambda x: [item.strip(string.punctuation).lower() for sublist\n",
    "                                                        in x for item in sublist])\n",
    "    dataframe['allwords'] = (dataframe['allwords'].apply(lambda x: [item for item in x if item.isalpha()\n",
    "                                                               and item not in stop_words]))\n",
    "    dataframe['mfreq'] = dataframe['allwords'].apply(nltk.FreqDist)\n",
    "    \n",
    "    dataframe['poslist'] = dataframe['pos'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "    dataframe['mfreqpos'] = dataframe['poslist'].apply(nltk.FreqDist)\n",
    "    \n",
    "    dataframe['stemwords'] = dataframe['words'].apply(lambda x: [pstemmer.stem(item) for sublist in x for item in sublist])\n",
    "    dataframe['stemwords'] = (dataframe['stemwords'].apply(lambda x: [item for item in x if item.isalpha()\n",
    "                                                               and item not in stop_words]))\n",
    "    dataframe['mfreqstem'] = dataframe['stemwords'].apply(nltk.FreqDist)\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score documents based on cleansed dataset - so should discount stopwords and be sensible\n",
    "def scoring(dataframe):\n",
    "    word_matches = defaultdict(list)\n",
    "    for word in keywords:\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            if word in row['allwords']:\n",
    "                dataframe.loc[idx, 'score'] += (row['mfreq'][word] * 0.75)\n",
    "                if not row['document'] in word_matches[word]:\n",
    "                    word_matches[word].append(row['document'])\n",
    "    print('\\n')\n",
    "    print('The following keyword hits occurred:')\n",
    "\n",
    "    for key, val in word_matches.items():\n",
    "        print(\"Keyword: \" + key + \". Found in these documents: \")\n",
    "        print(val)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score documents based on pos\n",
    "def scoringpos(dataframe):\n",
    "    word_matches = defaultdict(list)\n",
    "    for (w1, t1) in poskeywords:\n",
    "        for idx, row in dataframe.iterrows():\n",
    "             if (w1,t1) in row['poslist']:\n",
    "                    dataframe.loc[idx, 'score'] += row['mfreqpos'][(w1,t1)]\n",
    "                    if not row['document'] in word_matches[w1]:\n",
    "                        word_matches[w1].append(row['document'])\n",
    "    print('\\n')\n",
    "    print('The following keyword hits occurred:')\n",
    "\n",
    "    for key, val in word_matches.items():\n",
    "        print(\"Keyword: \" + key + \". Found in these documents: \")\n",
    "        print(val)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find keywords using POS\n",
    "def contextkeywords(dataframe):\n",
    "    print('\\n')\n",
    "    print('Here are the keywords in context: ')\n",
    "    # Search for IS as a noun\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        for index, r in enumerate(row['pos']):\n",
    "            for (w1, t1) in r:\n",
    "                if w1 == 'IS' and t1 == 'NNP':\n",
    "                    print(row['pos'][index])\n",
    "                    print('\\n')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort using a dirty model\n",
    "def dirtyscoring(dataframe):\n",
    "    dataframe['score2'] = 0\n",
    "    dataframe['w2'] = dataframe['words'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "    dataframe['mfreq2'] = dataframe['w2'].apply(nltk.FreqDist)\n",
    "\n",
    "    word_matches = defaultdict(list)\n",
    "    for word in keywords:\n",
    "        for idx, row in dataframe.iterrows():\n",
    "            if word in row['w2']:\n",
    "                dataframe.loc[idx, 'score2'] += row['mfreq2'][word]\n",
    "                if not row['document'] in word_matches[word]:\n",
    "                    word_matches[word].append(row['document'])\n",
    "    print('\\n')\n",
    "    print('The following keyword hits occurred in the uncleansed data:')\n",
    "\n",
    "    for key, val in word_matches.items():\n",
    "        print(\"Keyword: \" + key + \". Found in these documents: \")\n",
    "        print(val)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster documents and demonstrate prediction\n",
    "# TODO - calculate ideal k value\n",
    "def clustering(documents):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=0.2, use_idf=True,\n",
    "                                 tokenizer=tokenize_and_stem, ngram_range=(1, 3))\n",
    "    X = vectorizer.fit_transform(doclist)\n",
    "\n",
    "    true_k = 5\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    model.fit(X)\n",
    "\n",
    "    print(\"Top terms per cluster:\")\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i),\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind]),\n",
    "        print\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Prediction\")\n",
    "\n",
    "    Y = vectorizer.transform([\"this is a document about islamic state \"\n",
    "                              \"and terrorists and bombs IS jihad terrorism isil\"])\n",
    "    prediction = model.predict(Y)\n",
    "    print(\"A document with 'bad' terms would be in:\")\n",
    "    print(prediction)\n",
    "\n",
    "    Y = vectorizer.transform([\"completely innocent text just about kittens and puppies\"])\n",
    "    prediction = model.predict(Y)\n",
    "    print(\"A document with 'good' terms would be in:\")\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmflda(documentlist):\n",
    "    no_features = 1000\n",
    "\n",
    "    # NMF is able to use tf-idf\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(documentlist)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(documentlist)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    no_topics = 5\n",
    "\n",
    "    # Run NMF\n",
    "    nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "    # Run LDA\n",
    "    lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5,\n",
    "                                    learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "    no_top_words = 10\n",
    "    print(\"NMF Topics: \")\n",
    "    display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "    print(\"LDA Topics: \")\n",
    "    display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop function\n",
    "# Iterate over all files in the folder and process each one in turn\n",
    "print('Starting processing - the following files have been processed:')\n",
    "for input_file in glob.glob(os.path.join(input_path, '*.*')):\n",
    "    # Grab the file name\n",
    "    filename = os.path.basename(input_file)\n",
    "    fname = os.path.splitext(filename)[0]\n",
    "    print(filename)\n",
    "\n",
    "    # Parse the file to get to the text\n",
    "    parsed = parsewithtika(input_file)\n",
    "\n",
    "    # Language detection algorithm is non - deterministic, which means that if you try to run it on a text which is\n",
    "    # either too short or too ambiguous, you might get different results every time you run it\n",
    "    if filterlanguage(parsed):\n",
    "        continue\n",
    "\n",
    "    tokenised = tokenmakerwords(parsed)\n",
    "\n",
    "    # Ignore any documents with <50 words\n",
    "    if len(tokenised) < 100:\n",
    "        continue\n",
    "\n",
    "    # Create doclist for use in topic modelling\n",
    "    doclist.append(parsed)\n",
    "    # Sentence fragments\n",
    "    sentences = sent_tokenize(parsed)\n",
    "\n",
    "    # Build up dataframe\n",
    "    temp = pd.Series([filename, sentences])\n",
    "    d = d.append(temp, ignore_index=True)\n",
    "\n",
    "d.reset_index(drop=True, inplace=True)\n",
    "d.columns = ['document', 'sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenize the sentences, cleanup, parts of speech tagging\n",
    "wordtokens(d)\n",
    "d['score'] = 0\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scoring\n",
    "# TODO - use POS/stemming to make better counts of words, deal with cases\n",
    "scoring(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoringpos(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find words in context with POS\n",
    "contextkeywords(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by scoring\n",
    "d = d.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sorted documents\n",
    "print('\\n')\n",
    "print('Here are the scores based on cleansed data:')\n",
    "print(d[['document', 'score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirtyscoring(d)\n",
    "\n",
    "d = d.sort_values('score2', ascending=False)\n",
    "print('\\n')\n",
    "print('Here are the scores based on uncleansed data:')\n",
    "print(d[['document', 'score2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results of K Means Cluster and prediction modelling\n",
    "clustering(doclist)\n",
    "\n",
    "# Print results of NMF vs LDA topic modelling\n",
    "nmflda(doclist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
