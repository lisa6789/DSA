{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tika import parser\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document tokenization after text pre-preprocessing to differentiate types then token based on type\n",
    "\n",
    "input_path = 'C:\\\\test'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# have df be document, sentences, words, pos\n",
    "# do keyword searching from list\n",
    "# contextualise search using pos\n",
    "d = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tika to parse the file\n",
    "def parsewithtika(inputfile):\n",
    "    parsed = parser.from_file(inputfile)\n",
    "    # Extract the text content from the parsed file\n",
    "    psd = parsed[\"content\"]\n",
    "    return re.sub(r'\\s+', ' ', psd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenmakerwords(inputfile):\n",
    "    # Create tokens\n",
    "    tokens = word_tokenize(inputfile)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    stripped = [w.strip(string.punctuation) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    text = nltk.Text(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language filter\n",
    "def filterlanguage(inputfile):\n",
    "    if detect(inputfile) != 'en':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokens, parts of speech tagging\n",
    "def wordtokens(dataframe):\n",
    "    dataframe['words'] = (dataframe['sentences'].apply(lambda x: [word_tokenize(item.strip(string.punctuation).lower())\n",
    "                                                                  for item in x]))\n",
    "    dataframe['words'] = (dataframe['words'].apply(lambda x: [[item for item in lst if item.isalpha()\n",
    "                                                               and item not in stop_words] for lst in x]))\n",
    "    dataframe['pos'] = dataframe['words'].apply(lambda x: [nltk.pos_tag(item) for item in x])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "031918comments2.authcheckdam.pdf\n",
      "881961_CHECKLIST-2014_rev62714.pdf\n",
      "DomesticWireFunds.pdf\n",
      "Order Confirmation.pdf\n",
      "Orderconf.pdf\n",
      "Patient+Type+2+opt-out+letter+v1.0.pdf\n",
      "r003.pdf\n",
      "Sample Grade and Receipt Documents.pdf\n",
      "START_AM2014_QuickFireTwo.pdf\n",
      "START_ECDB_ViolencePerpetratedbySupportersofAQAM_June2014.pdf\n",
      "START_ISIL_Lesson1_ObjectivesScenariosforISIL.pdf\n",
      "START_TranscendingOrganizationIndividualsandtheIslamicState_AnalyticalBrief_June2014.pdf\n",
      "water-companies-letter-SoS-to-Ofwat-180131.pdf\n"
     ]
    }
   ],
   "source": [
    "# Main loop function\n",
    "# Iterate over all files in the folder and process each one in turn\n",
    "for input_file in glob.glob(os.path.join(input_path, '*.*')):\n",
    "    # Grab the file name\n",
    "    filename = os.path.basename(input_file)\n",
    "    fname = os.path.splitext(filename)[0]\n",
    "    print(filename)\n",
    "\n",
    "    # Parse the file to get to the text\n",
    "    parsed = parsewithtika(input_file)\n",
    "\n",
    "    # Language detection algorithm is non - deterministic, which means that if you try to run it on a text which is\n",
    "    # either too short or too ambiguous, you might get different results every time you run it\n",
    "    if filterlanguage(parsed):\n",
    "        continue\n",
    "\n",
    "    tokenised = tokenmakerwords(parsed)\n",
    "\n",
    "    # Ignore any documents with <50 words\n",
    "    if len(tokenised) < 100:\n",
    "        continue\n",
    "\n",
    "    # Sentence fragments\n",
    "    sentences = sent_tokenize(parsed)\n",
    "\n",
    "    # Build up dataframe\n",
    "    temp = pd.Series([filename, sentences])\n",
    "    d = d.append(temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.reset_index(drop=True,inplace=True)\n",
    "d.columns = ['document', 'sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>031918comments2.authcheckdam.pdf</td>\n",
       "      <td>[ Section of Taxation Suite 400 1050 Connectic...</td>\n",
       "      <td>[[section, taxation, suite, connecticut, avenu...</td>\n",
       "      <td>[[(section, NN), (taxation, NN), (suite, NN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>881961_CHECKLIST-2014_rev62714.pdf</td>\n",
       "      <td>[ CHECKLIST-2014_rev62714 ORDER CONFIRMATION C...</td>\n",
       "      <td>[[order, confirmation, checklistorder, confirm...</td>\n",
       "      <td>[[(order, NN), (confirmation, NN), (checklisto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DomesticWireFunds.pdf</td>\n",
       "      <td>[ ►►►►►PLEASE PRINT◄◄◄◄◄ WIRE TRANSFER PAYMENT...</td>\n",
       "      <td>[[wire, transfer, payment, order, confirmation...</td>\n",
       "      <td>[[(wire, NN), (transfer, NN), (payment, NN), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Order Confirmation.pdf</td>\n",
       "      <td>[ Microsoft Word - Order Confirmation.doc Orde...</td>\n",
       "      <td>[[microsoft, word, order, order, confirmation,...</td>\n",
       "      <td>[[(microsoft, JJ), (word, NN), (order, NN), (o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patient+Type+2+opt-out+letter+v1.0.pdf</td>\n",
       "      <td>[ (Title) (First name) (Surname) (Address line...</td>\n",
       "      <td>[[title, first, name, surname, address, line, ...</td>\n",
       "      <td>[[(title, NN), (first, RB), (name, JJ), (surna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>r003.pdf</td>\n",
       "      <td>[ 1 Von: auto-confirm@amazon.co.uk Gesendet: S...</td>\n",
       "      <td>[[von, gesendet, samstag], [juli, betreff, ord...</td>\n",
       "      <td>[[(von, NN), (gesendet, NN), (samstag, NN)], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sample Grade and Receipt Documents.pdf</td>\n",
       "      <td>[ Page 1 Sample Grade and Receipt Documents Ce...</td>\n",
       "      <td>[[page, sample, grade, receipt, documents, cen...</td>\n",
       "      <td>[[(page, NN), (sample, NN), (grade, VBD), (rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>START_AM2014_QuickFireTwo.pdf</td>\n",
       "      <td>[ U.S. Attitudes toward Terrorism and Countert...</td>\n",
       "      <td>[[attitudes, toward, terrorism, counterterrori...</td>\n",
       "      <td>[[(attitudes, NNS), (toward, IN), (terrorism, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>START_ECDB_ViolencePerpetratedbySupportersofAQ...</td>\n",
       "      <td>[ National Consortium for the Study of Terrori...</td>\n",
       "      <td>[[national, consortium, study, terrorism, resp...</td>\n",
       "      <td>[[(national, JJ), (consortium, NN), (study, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>START_ISIL_Lesson1_ObjectivesScenariosforISIL.pdf</td>\n",
       "      <td>[ Microsoft Word - U_SMA SOCCENT White Paper F...</td>\n",
       "      <td>[[microsoft, word, soccent, white, paper, fina...</td>\n",
       "      <td>[[(microsoft, JJ), (word, NN), (soccent, NN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>START_TranscendingOrganizationIndividualsandth...</td>\n",
       "      <td>[ START Analytical Brief © START, June 2014 1 ...</td>\n",
       "      <td>[[start, analytical, brief, start, june, senio...</td>\n",
       "      <td>[[(start, RB), (analytical, JJ), (brief, JJ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>water-companies-letter-SoS-to-Ofwat-180131.pdf</td>\n",
       "      <td>[ The Rt Hon Michael Gove MP From the Secretar...</td>\n",
       "      <td>[[rt, hon, michael, gove, mp, secretary, state...</td>\n",
       "      <td>[[(rt, NN), (hon, NN), (michael, NN), (gove, V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             document  \\\n",
       "0                    031918comments2.authcheckdam.pdf   \n",
       "1                  881961_CHECKLIST-2014_rev62714.pdf   \n",
       "2                               DomesticWireFunds.pdf   \n",
       "3                              Order Confirmation.pdf   \n",
       "4              Patient+Type+2+opt-out+letter+v1.0.pdf   \n",
       "5                                            r003.pdf   \n",
       "6              Sample Grade and Receipt Documents.pdf   \n",
       "7                       START_AM2014_QuickFireTwo.pdf   \n",
       "8   START_ECDB_ViolencePerpetratedbySupportersofAQ...   \n",
       "9   START_ISIL_Lesson1_ObjectivesScenariosforISIL.pdf   \n",
       "10  START_TranscendingOrganizationIndividualsandth...   \n",
       "11     water-companies-letter-SoS-to-Ofwat-180131.pdf   \n",
       "\n",
       "                                            sentences  \\\n",
       "0   [ Section of Taxation Suite 400 1050 Connectic...   \n",
       "1   [ CHECKLIST-2014_rev62714 ORDER CONFIRMATION C...   \n",
       "2   [ ►►►►►PLEASE PRINT◄◄◄◄◄ WIRE TRANSFER PAYMENT...   \n",
       "3   [ Microsoft Word - Order Confirmation.doc Orde...   \n",
       "4   [ (Title) (First name) (Surname) (Address line...   \n",
       "5   [ 1 Von: auto-confirm@amazon.co.uk Gesendet: S...   \n",
       "6   [ Page 1 Sample Grade and Receipt Documents Ce...   \n",
       "7   [ U.S. Attitudes toward Terrorism and Countert...   \n",
       "8   [ National Consortium for the Study of Terrori...   \n",
       "9   [ Microsoft Word - U_SMA SOCCENT White Paper F...   \n",
       "10  [ START Analytical Brief © START, June 2014 1 ...   \n",
       "11  [ The Rt Hon Michael Gove MP From the Secretar...   \n",
       "\n",
       "                                                words  \\\n",
       "0   [[section, taxation, suite, connecticut, avenu...   \n",
       "1   [[order, confirmation, checklistorder, confirm...   \n",
       "2   [[wire, transfer, payment, order, confirmation...   \n",
       "3   [[microsoft, word, order, order, confirmation,...   \n",
       "4   [[title, first, name, surname, address, line, ...   \n",
       "5   [[von, gesendet, samstag], [juli, betreff, ord...   \n",
       "6   [[page, sample, grade, receipt, documents, cen...   \n",
       "7   [[attitudes, toward, terrorism, counterterrori...   \n",
       "8   [[national, consortium, study, terrorism, resp...   \n",
       "9   [[microsoft, word, soccent, white, paper, fina...   \n",
       "10  [[start, analytical, brief, start, june, senio...   \n",
       "11  [[rt, hon, michael, gove, mp, secretary, state...   \n",
       "\n",
       "                                                  pos  \n",
       "0   [[(section, NN), (taxation, NN), (suite, NN), ...  \n",
       "1   [[(order, NN), (confirmation, NN), (checklisto...  \n",
       "2   [[(wire, NN), (transfer, NN), (payment, NN), (...  \n",
       "3   [[(microsoft, JJ), (word, NN), (order, NN), (o...  \n",
       "4   [[(title, NN), (first, RB), (name, JJ), (surna...  \n",
       "5   [[(von, NN), (gesendet, NN), (samstag, NN)], [...  \n",
       "6   [[(page, NN), (sample, NN), (grade, VBD), (rec...  \n",
       "7   [[(attitudes, NNS), (toward, IN), (terrorism, ...  \n",
       "8   [[(national, JJ), (consortium, NN), (study, NN...  \n",
       "9   [[(microsoft, JJ), (word, NN), (soccent, NN), ...  \n",
       "10  [[(start, RB), (analytical, JJ), (brief, JJ), ...  \n",
       "11  [[(rt, NN), (hon, NN), (michael, NN), (gove, V...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenize the sentences, cleanup, parts of speech tagging\n",
    "wordtokens(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 document  \\\n",
      "0        031918comments2.authcheckdam.pdf   \n",
      "1      881961_CHECKLIST-2014_rev62714.pdf   \n",
      "2                   DomesticWireFunds.pdf   \n",
      "3                  Order Confirmation.pdf   \n",
      "4  Patient+Type+2+opt-out+letter+v1.0.pdf   \n",
      "\n",
      "                                           sentences  \\\n",
      "0  [ Section of Taxation Suite 400 1050 Connectic...   \n",
      "1  [ CHECKLIST-2014_rev62714 ORDER CONFIRMATION C...   \n",
      "2  [ ►►►►►PLEASE PRINT◄◄◄◄◄ WIRE TRANSFER PAYMENT...   \n",
      "3  [ Microsoft Word - Order Confirmation.doc Orde...   \n",
      "4  [ (Title) (First name) (Surname) (Address line...   \n",
      "\n",
      "                                               words  \\\n",
      "0  [[section, taxation, suite, connecticut, avenu...   \n",
      "1  [[order, confirmation, checklistorder, confirm...   \n",
      "2  [[wire, transfer, payment, order, confirmation...   \n",
      "3  [[microsoft, word, order, order, confirmation,...   \n",
      "4  [[title, first, name, surname, address, line, ...   \n",
      "\n",
      "                                                 pos  \\\n",
      "0  [[(section, NN), (taxation, NN), (suite, NN), ...   \n",
      "1  [[(order, NN), (confirmation, NN), (checklisto...   \n",
      "2  [[(wire, NN), (transfer, NN), (payment, NN), (...   \n",
      "3  [[(microsoft, JJ), (word, NN), (order, NN), (o...   \n",
      "4  [[(title, NN), (first, RB), (name, JJ), (surna...   \n",
      "\n",
      "                                               mfreq  \n",
      "0  [{'section': 1, 'taxation': 1, 'suite': 1, 'co...  \n",
      "1  [{'order': 2, 'confirmation': 2, 'checklistord...  \n",
      "2  [{'wire': 1, 'transfer': 2, 'payment': 3, 'ord...  \n",
      "3  [{'microsoft': 1, 'word': 1, 'order': 2, 'conf...  \n",
      "4  [{'title': 1, 'first': 1, 'name': 1, 'surname'...  \n"
     ]
    }
   ],
   "source": [
    "print(d.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['mfreq'] = d['words'].apply(lambda x: [nltk.FreqDist(item) for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'most_common'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-620402a32531>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mfreq'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'{};{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\DSA\\venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'most_common'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for word, frequency in d['mfreq'].most_common(50):\n",
    "    print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
